{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-AgNchSd_hu"
   },
   "source": [
    "# Teleseismic *P*-wave coda autocorrelation\n",
    "\n",
    "[![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day4/D4_Lab.ipynb)\n",
    "\n",
    "Thanh-Son Pham (ThanhSon.Pham@anu.edu.au)\n",
    "\n",
    "- October 2017: First release on [GitHub](https://github.com/tsonpham/PCodaAutocorrelation.git).\n",
    "- March 2025: Revision for Observational Seismology Workshop at VNU Hanoi University of Science.\n",
    "\n",
    "This notebook implements a lightweight version of the *P wave coda autocorrelation* method in the Python programing language. The method is demonstrated with seismic data acquired by station [YT ST01](https://ds.iris.edu/mda/YT/ST01/?starttime=2010-01-27T00:00:00&endtime=2012-12-31T23:59:59) operating from 2010 to 2012 over the ~3 km thick icesheet in West Antarctica.\n",
    "\n",
    "For more details, we refer to the following articles:\n",
    "- Phạm T.-S. and H. Tkalčić, *On the feasibility and use of teleseismic P-wave coda autocorrelation for mapping shallow seismic discontinuities*, J. Geophys. Res.: Solid Earth, 122, doi:10.1002/2017JB013975, **2017**.\n",
    "- Phạm T.-S. and H. Tkalčić, *Antarctic ice properties revealed from teleseismic P wave coda autocorrelation*, J. Geophys. Res.: Solid Earth, 123, doi:10.1029/2018JB016115, **2018**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnhqKU_vd_hy"
   },
   "source": [
    "---\n",
    "## What we do in this notebook\n",
    "Here we demonstrate the *P-wave coda autocorrelation method* to image the ice-bedrock interface beneath the Antarctic ice sheet using single-component seismograms. Specific tasks include:\n",
    "* Retrieve automatically waveforms of a seismic phase from the IRIS data server,\n",
    "* Perform pre-processing on retrieved waveforms,\n",
    "* Compute the autocorrelation function of time series,\n",
    "* Apply bandpass filter to the computed autocorrelograms,\n",
    "* Interprete the autocorrelogram in the glacial context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7440,
     "status": "ok",
     "timestamp": 1745302303658,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "oYWtl1BVd_hy",
    "outputId": "734aeed5-a2f6-4ca2-ad26-c5cb26db772b"
   },
   "outputs": [],
   "source": [
    "# Environemtal setup (uncomment if running in colab)\n",
    "\n",
    "# !pip install obspy basemap numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745302303684,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "I5rgTGn4d_h0"
   },
   "outputs": [],
   "source": [
    "#@title Notebook visualisation settings\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"savefig.dpi\"] = 100\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "rcParams[\"font.size\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phjIEYcVHQuW"
   },
   "source": [
    "---\n",
    "## Cross-correlation function (CCF)\n",
    "\n",
    "For two discrete time series (seismograms) consisting of $N$ points, ${x(n); n = 0, \\ldots, N-1}$ and ${y(n); n = 0, \\ldots, N-1}$, the cross-correlation function $v(m)$ has $2N+1$ points ${v(m);m=-N+1,N-1}$ and is defined as\n",
    "$$\n",
    "v(m)= \\sum_{0 \\le n < N \\& 0 \\le m+n < N} x(n) \\cdot y(m + n) \\; (1)\n",
    "$$\n",
    "The discrete variable $m$ connects a point in time series $x$ with a point in time series $y$. The cross-correlation is then the inner product in the overlap of the two time-series.\n",
    "\n",
    "As well as computing a sliding summation in the time domain, cross-correlation can also be computed using the frequency domain. Let $\\hat{x}$ and $\\hat{y}$ be the discrete Fourier transforms of two input time signals, $x, y$. In the frequency domain, the temporal cross-correlation function (as expressed in Equation 1) is equivalent to the inverse Fourier transform of the point-wise multiplication, of one transform with the complex conjugate of the other\n",
    "$$\n",
    "\\hat{v}(\\omega)=\\hat{x}(\\omega) \\cdot \\hat{y}^*(\\omega). \\; (2)\n",
    "$$\n",
    "When cross-correlating time series with large $N$ (such as seismograms), a spectral operation is more efficient than its temporal counterpart due to the existence of Fast Fourier Transform (FFT) which allows rapid computation of Fourier transforms [Cooley and Tukey, 1965].\n",
    "\n",
    "To obtain an identical result as in the temporal cross-correlation (Equation 1), one must increase the size of input signals $x,y$ to $(2N+1)$ by padding zeros before the Fourier transform is applied. This zero-padding is used in our data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 1428,
     "status": "ok",
     "timestamp": 1745302305114,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "_GagbFdXHQuW",
    "outputId": "1633a567-7255-41c7-d977-b78e3c9e6e3a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Two random time series of length N\n",
    "N = 100\n",
    "x = np.random.normal(0, 1, N)\n",
    "y = np.random.normal(0, 1, N)\n",
    "\n",
    "# Cross-correlation in time domain as in Equation 1\n",
    "ccf_1 = np.correlate(x, y, mode='full')\n",
    "\n",
    "# Cross-correlation in frequency domain as in Equation 2\n",
    "x_f = np.fft.fft(x, len(x) + len(y) - 1)\n",
    "y_f = np.fft.fft(y, len(x) + len(y) - 1)\n",
    "ccf_2 = np.fft.fftshift(np.fft.ifft(x_f * y_f.conjugate()).real)\n",
    "\n",
    "# Plot the cross-correlation results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1-N, N), ccf_1, label='Time domain CCF', lw=2)\n",
    "ax.plot(np.arange(1-N, N), ccf_2, label='Frequency domain CCF', lw=.75)\n",
    "ax.legend()\n",
    "ax.set(xlabel='Time Lag', ylabel='CCF')\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_5HySBoHQuX"
   },
   "source": [
    "---\n",
    "## Autocorrelation function (ACF)\n",
    "\n",
    "Autocorrelation function is the cross-correlation of a time series with itself.\n",
    "$$\n",
    "a(m)= \\sum_{0 \\le n < N \\& 0 \\le m+n < N} x(n) \\cdot x(m + n) \\; (1)\n",
    "$$\n",
    "In the spectral autocorrelation function is the product of the input spectrum and its conjugate, thus it has zero phase\n",
    "$$\n",
    "\\hat{a}(\\omega)=\\hat{x}(\\omega) \\cdot \\hat{x}^*(\\omega) = |\\hat{x}(\\omega)|^2. \\; (2)\n",
    "$$\n",
    "In the time domain, the autocorrelation is symmetric over the central peak. The amplitude of the central peak equals to the power of the input time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1745302305473,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "-Vvyzjq8HQuX",
    "outputId": "9c93214d-b3bf-4e01-adcd-0fea987f9045"
   },
   "outputs": [],
   "source": [
    "# Cross-correlation in time domain as in Equation 1\n",
    "acf_1 = np.correlate(x, x, mode='full')\n",
    "\n",
    "# Cross-correlation in frequency domain as in Equation 2\n",
    "x_f = np.fft.fft(x, 2*len(x) - 1)\n",
    "acf_2 = np.fft.fftshift(np.fft.ifft(x_f * x_f.conjugate()).real)\n",
    "\n",
    "# Plot the cross-correlation results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1-N, N), acf_1, label='Time domain ACF', lw=2)\n",
    "ax.plot(np.arange(1-N, N), acf_2, label='Frequency domain ACF', lw=.75)\n",
    "ax.legend()\n",
    "ax.set(xlabel='Time Lag', ylabel='ACF')\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie2RwU2Vd_h0"
   },
   "source": [
    "---\n",
    "## Data processing overview\n",
    "\n",
    "The autocorrelation method include two major steps.\n",
    "\n",
    "**STEP 1**\n",
    "\n",
    "As sumarised in the figure below, an individidual P-wave coda record is transformed into the frequency domain. The spectrum is balanced, or whitened, using an adaptive weighting trace. The autocorrelation is computed by squaring the whitened spectrum, then inversely tranformed to the time domain (figure below).\n",
    "\n",
    "<div>\n",
    "<img src=\"https://agupubs.onlinelibrary.wiley.com/cms/asset/5fab2346-82da-41a5-a4b9-68965dee2cea/jgrb52088-fig-0004-m.jpg\" width=\"400\"/>\n",
    "</div>\n",
    "(Source: Phạm & Tkalčić, 2017, JGR)\n",
    "\n",
    "**STEP 2**\n",
    "\n",
    "Autorrelograms of teleseismic earthquakes to the same station are stacked, or summed up, to improve the signal to noise ratio of the stacked autocorrelogram. We use linear stacking (DLS) or phase weighted stacking (PWS) methods to complement the interpretation.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://agupubs.onlinelibrary.wiley.com/cms/asset/389caf59-989c-424a-81be-74a6e6d13350/jgrb53027-fig-0002-m.jpg\" width=\"400\"/>\n",
    "</div>\n",
    "(Source: Phạm & Tkalčić, 2018, JGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpI2B3hYd_h0"
   },
   "source": [
    "### Spectral normalization\n",
    "We use an adaptive weighting function to normalize the complex spectrum of the input waveform before computing the autocorrelation function. The spectral whitening is defined in a similar way to the running-absolute-mean normalization [Bensen et al., 2007] in the time domain as\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{s}_n = \\frac{s_n} {\\frac{1}{2N + 1} \\sum_{j=n-N}^{n+N} |s_j|}.\n",
    "\\end{equation}\n",
    "\n",
    "The number of averaging points $N$ in the denominator is alternatively refered as spectral whitening width $\\Delta W = 2 N \\Delta \\omega$, where $\\Delta \\omega$ is the discrete frequency step $\\Delta \\omega = 1/L$ and $L$ is the length of the input time trace. Tuning the whitening width $\\Delta W$ may change the performance of the method. We recommend to do experiments with this parameter to find an optimal value for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1745302305709,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "B4siP8Kbd_h1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import fftconvolve, hilbert\n",
    "\n",
    "def smooth_avg(arr, winlen):\n",
    "    \"\"\"\n",
    "    Smoothing a time series by averaging its values in a moving window. This averaging\n",
    "    window operation can be done efficiently with an convolution in the frequency domain.\n",
    "    \"\"\"\n",
    "\n",
    "    window = np.ones(winlen) / (1.0 * winlen)\n",
    "    return fftconvolve(arr, window, 'same')\n",
    "\n",
    "def spectral_whitening(arr, delta, freq_width, npts=None, returnweight=False):\n",
    "    \"\"\"\n",
    "    Frequency whitening of a time series by balancing its spectrum by smoothed absolute\n",
    "    array of the spectrum.\n",
    "\n",
    "    arr        : input seismogram\n",
    "    delta      : sampling time step of the input\n",
    "    freq_width : spectral whitening width\n",
    "    returnweith: return the averaging width or not\n",
    "    \"\"\"\n",
    "    npts = len(arr) if npts is None else npts\n",
    "    carr = np.fft.fftshift(np.fft.fft(arr, 2 * npts))\n",
    "\n",
    "    Nyfreq = 0.5 / delta\n",
    "    spec_step = Nyfreq / npts\n",
    "    if freq_width != None:\n",
    "        winlen = int(0.5 * freq_width / spec_step) * 2 + 1\n",
    "        weight = smooth_avg(np.abs(carr), winlen)\n",
    "        if any(weight < 1e-8): raise Exception('Zero division')\n",
    "\n",
    "        carr /= weight\n",
    "        carr[weight<1e-6*np.max(weight)] = 0\n",
    "\n",
    "    if returnweight:\n",
    "        return carr[npts-1:2*npts], weight[npts-1:2*npts]\n",
    "    else:\n",
    "        return carr[npts-1:2*npts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3saVLuvfd_h1"
   },
   "source": [
    "### Autocorrelation and bandpass filter\n",
    "The autocorrelation of a time trace is the inverse transform of the whitend power spectrum.\n",
    "$$\n",
    "a_n(\\omega) = |\\hat{s}_n|^2\n",
    "$$\n",
    "The discrete spectrum of the autocorrelation in the frequemcy domain, $a_n(\\omega)$ is inverse Fourier transformed to the time domain.\n",
    "\n",
    "The only causal part of the autocorrelation trace is retained. We use a cosine taper to the central peak and bandpass in the frequency band 1-5 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1745302305724,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "j9PjgRaSd_h1"
   },
   "outputs": [],
   "source": [
    "def compute_auto(tr, freq_width, npts=None,\n",
    "                 filter_kwargs={'freqmin':1.0, 'freqmax':5.0, 'corners':4, 'zerophase':True},\n",
    "                 taper_width=0.5):\n",
    "    \"\"\"\n",
    "    Computing the autocorrelation function of the whitened trace.\n",
    "\n",
    "    tr: input data seismogram\n",
    "    freq_width: spectral whitening width\n",
    "    npts: number of points for the autocorrelation\n",
    "    filter_kwargs: filtering parameters\n",
    "    taper_width: tapering width in seconds\n",
    "    \"\"\"\n",
    "    npts = tr.stats.npts if npts is None else npts\n",
    "    ## whitening and autocorrelating\n",
    "    spectrum = spectral_whitening(tr.data, tr.stats.delta, freq_width, npts)\n",
    "    ## autocorrelating\n",
    "    tr.data = np.fft.irfft(np.abs(spectrum)**2)[0:npts]\n",
    "\n",
    "    # post processing: tapering and filtering\n",
    "    taper_frac = taper_width / (npts * tr.stats.delta)\n",
    "    tr.taper(type='cosine', max_percentage=taper_frac)\n",
    "    tr.filter('bandpass', **filter_kwargs)\n",
    "    tr.taper(type='cosine', max_percentage=taper_frac)\n",
    "\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_avGYcJ3d_h2"
   },
   "source": [
    "### Phase weighted stacking\n",
    "\n",
    "*Phase-weighted stacking* is a nonlinear stacking technique to sufficiently enhance coherent signals and suppress noise [Schimmel and Paulssen, 1997]. It uses the averaging phase of the input traces to weight the linear stack of the input.\n",
    "\n",
    "The analytical signals of a one-sided autocorrelogram $s_n$ is given by\n",
    "$$\n",
    "S_n(t) = s_n(t) + i H_n(t) = A_n(t) e^{i\\Phi_n(t)}\n",
    "$$\n",
    "where $H_n(t)$ is the Hilbert transform of the original trace $s_n(t)$. And, $A_n(t)$, $\\Phi_n(t)$ are the amplitude and phase components of the analytic signal. The phase-weighted stack of $N$ input traces is defined as\n",
    "$$\n",
    "g(t) = \\frac{1}{N} \\sum_{n=1}^N s_n(t) \\; \\left|\\frac{1}{N}\\sum_{n=1}^N e^{i\\Phi_n(t)}\\right|^\\eta.\n",
    "$$\n",
    "The order $\\eta$ is used to adjust the importance of the phase weighting factor. When $\\eta = 0$, the 0th order phase-weighted stack becomes a linear stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1745302306302,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "uvubmHWRd_h2"
   },
   "outputs": [],
   "source": [
    "from obspy.core import Trace\n",
    "\n",
    "def data_stack(stream, order):\n",
    "    \"\"\"\n",
    "    Phase weighted stacking\n",
    "\n",
    "    stream: input stream of one-sided autocorrelograms\n",
    "    order : order of the weight phase\n",
    "    \"\"\"\n",
    "    stack = 0\n",
    "    phase = 0j\n",
    "\n",
    "    if order == 0:\n",
    "        stack = np.sum([acorr.data for acorr in stream], axis=0)\n",
    "        return Trace(header={'npts': len(stack), 'delta': stream[0].stats.delta}, data=stack)\n",
    "\n",
    "    for acorr in stream:\n",
    "        stack += acorr.data\n",
    "\n",
    "        ## calculate phase\n",
    "        asig = hilbert(acorr.data)\n",
    "        phase += asig / np.abs(asig)\n",
    "\n",
    "    stack /= len(stream)\n",
    "    weight = np.abs(phase / len(stream))\n",
    "\n",
    "    return Trace(header={'npts': len(stack), 'delta': stream[0].stats.delta}, data=stack * weight**order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_us01E4nd_h2"
   },
   "source": [
    "---\n",
    "## Application for pilot station ST01 (West Antarctica)\n",
    "\n",
    "This section contains step-by-step guide to acquire seismic data from remote server, perform processing, visualise data, and make initial interpretation of the results in the geological context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVzLEyfyd_h2"
   },
   "source": [
    "#### Data preparation\n",
    "\n",
    "First, we query the geographical location of station ST01 from IRIS Data Management center using an `obspy fdsn client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1745302307394,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "iakiMaiod_h2",
    "outputId": "49b6f370-1644-4450-d903-96ace80ffac6"
   },
   "outputs": [],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "## definte the IRIS client\n",
    "iris = Client('IRIS')\n",
    "## name a station in Antarctica\n",
    "stacode = 'ST01'\n",
    "## get station information\n",
    "inv = iris.get_stations(network='_ANTARCTICA', station=stacode, channel='?HZ', level='channel')\n",
    "## some station names are reused for diffent temporary network campaigns\n",
    "print (inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XRWPTHfd_h3"
   },
   "source": [
    "Now, plot the station map for sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1745302308409,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "emB8oJyAd_h3",
    "outputId": "c707eb2c-a1ef-4463-a336-a594ea8d751b"
   },
   "outputs": [],
   "source": [
    "def plot_station_map(inv):\n",
    "    \"\"\"\n",
    "    Plotting the station location from the inventory in the Antarctic map.\n",
    "    \"\"\"\n",
    "    m = Basemap(projection='spstere', boundinglat=-65, lon_0=180, resolution='l')\n",
    "    m.drawcoastlines()\n",
    "    for net in inv:\n",
    "        for sta in net:\n",
    "            m.plot(sta.longitude, sta.latitude, 'r*', markersize=10, latlon=True)\n",
    "    plt.show()\n",
    "# call the function to plot map\n",
    "plot_station_map(inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFmf849_d_h4"
   },
   "source": [
    "Second, search for all catalogued earthquakes of magnitude Mw 6.0+, between distances from 30 to 95° from the seismic station of interest, and download data for the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27426,
     "status": "ok",
     "timestamp": 1745302335833,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "i7pYMz9dd_h4",
    "outputId": "39550d4c-9126-470a-82d1-af9e738f5ea1"
   },
   "outputs": [],
   "source": [
    "from obspy.taup import TauPyModel\n",
    "from obspy.geodetics import gps2dist_azimuth\n",
    "from obspy import Catalog\n",
    "\n",
    "taup_model = TauPyModel(model='ak135')\n",
    "def download_data(inv, min_magnitude=6.0, min_radius=30, max_radius=95,\n",
    "        sample_rate=40, phase = 'P', before=10, after=50,\n",
    "        cha_priorities=['BHZ', 'HHZ'], loc_priorities=['', '00', '10'],\n",
    "        minlength=0.95, bulk_size=100):\n",
    "    '''\n",
    "    Download the waveform data from IRIS DMC for the given station inventory.\n",
    "\n",
    "    This function helps to deal with some complicated situations for example,\n",
    "    when same station name is used in two networksthe catalogues are concatenated\n",
    "    to access the full data availability\n",
    "    '''\n",
    "    bulk_list = []\n",
    "    metadata = []\n",
    "    ## get the event catalog in the desired distance range during the station operation\n",
    "    for net in inv:\n",
    "        for sta in net:\n",
    "            ## get the chanel code from the priority list\n",
    "            for chacode in cha_priorities:\n",
    "                if chacode in [cha.code for cha in sta]: break\n",
    "            if chacode not in [cha.code for cha in sta]: continue\n",
    "            ## get the location code from the priority list given the channel code\n",
    "            for loccode in loc_priorities:\n",
    "                if loccode in [cha.location_code for cha in sta if cha.code == chacode]: break\n",
    "            if loccode not in [cha.location_code for cha in sta if cha.code == chacode]: continue\n",
    "            ## get the event catalog in the desired distance range during the station operation\n",
    "            cat = iris.get_events(starttime=sta.start_date, endtime=sta.end_date,\n",
    "                latitude=sta.latitude, longitude=sta.longitude,\n",
    "                minradius=min_radius, maxradius=max_radius, minmagnitude=min_magnitude)\n",
    "            ## create a list to store the bulk information\n",
    "            for event in cat:\n",
    "                org = event.preferred_origin()\n",
    "                ## calculate distance, azimuth, and back-azimuth\n",
    "                dist, az, baz = gps2dist_azimuth(org.latitude, org.longitude, sta.latitude, sta.longitude)\n",
    "                ## calculate the P-wave arrival time from the event to the station\n",
    "                p_arv = taup_model.get_travel_times_geo(org.depth/1e3, org.latitude, org.longitude,\n",
    "                                                        sta.latitude, sta.longitude, [phase])[0]\n",
    "                p_arv_time = org.time + p_arv.time\n",
    "                ## enclose the calculated information into list\n",
    "                bulk_list.append((net.code, sta.code, loccode, chacode, p_arv_time-before, p_arv_time+after))\n",
    "                metadata.append({'dist': dist/1e3, 'az': az, 'baz': baz, 'p_atime': p_arv_time,\n",
    "                                 'evla': org.latitude, 'evlo': org.longitude, 'evmag': event.magnitudes[0].mag,\n",
    "                                 'evdp': org.depth/1e3, 'stla': sta.latitude, 'stlo': sta.longitude})\n",
    "    ## query the waveform data from server\n",
    "    if bulk_size is None or bulk_size>len(bulk_list):\n",
    "        dstream = iris.get_waveforms_bulk(bulk=bulk_list, threaded=True, minimumlength=minlength*(before+after))\n",
    "    else:\n",
    "        idx = np.random.choice(len(bulk_list), bulk_size, False)\n",
    "        dstream = iris.get_waveforms_bulk(bulk=[bulk_list[_] for _ in idx], threaded=True,\n",
    "                                          minimumlength=minlength*(before+after))\n",
    "    ## associate retieved waveforms with the event information\n",
    "    p_atimes = np.array([_['p_atime'] for _ in metadata])\n",
    "    for tr in dstream:\n",
    "        idx = np.where(np.logical_and(p_atimes < tr.stats.endtime, p_atimes > tr.stats.starttime))[0]\n",
    "        if len(idx) == 0:\n",
    "            dstream.remove(tr)\n",
    "        else:\n",
    "            tr.stats.update(metadata[idx[0]])\n",
    "    ## resample the data to 40 Hz (if necessary) and remove the linear data trend\n",
    "    dstream.resample(sample_rate)\n",
    "    dstream.detrend('linear')\n",
    "    return dstream\n",
    "\n",
    "## evaluate the function to download the data\n",
    "dstream = download_data(inv, bulk_size=100)\n",
    "print (dstream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNWfrNMvd_h4"
   },
   "source": [
    "Let's plot the event map to see where they are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1745302336723,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "mIpwc9uhd_h4",
    "outputId": "32cb1a94-a07e-46be-80ed-3aef23569775"
   },
   "outputs": [],
   "source": [
    "## plot the event location on a global map\n",
    "m = Basemap(projection='robin', lon_0=0, resolution='c')\n",
    "m.drawcoastlines(linewidth=.75)\n",
    "## plot event location by red stars\n",
    "evla = [tr.stats.evla for tr in dstream.select(component='Z')]\n",
    "evlo = [tr.stats.evlo for tr in dstream.select(component='Z')]\n",
    "m.scatter(evlo, evla, latlon=True, s=10, c='r', marker='*')\n",
    "## plot the station location by blue triangle\n",
    "stla, stlo = dstream[0].stats.stla, dstream[0].stats.stlo\n",
    "m.plot(stlo, stla, 'b^', markersize=10, latlon=True)\n",
    "## plot the tissots showing the distance from the station\n",
    "_lats = np.linspace(-90, 90, 100)\n",
    "_lons = np.linspace(-180, 180, 100)\n",
    "lons, lats = np.meshgrid(_lons, _lats)\n",
    "from obspy.geodetics import locations2degrees\n",
    "gcarc = locations2degrees(lats, lons, stla, stlo)\n",
    "cs = m.contour(lons, lats, gcarc, levels=[30, 95], colors='k', latlon=True)\n",
    "# labels the countour inline with some custom format\n",
    "plt.clabel(cs, fmt='%2.0f$^\\circ$', inline=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRXDPTPyd_h5"
   },
   "source": [
    "#### Processing and interpretation\n",
    "\n",
    "Then, we autocorrelate individual event seismograms and stack them up to improve signal-to-noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 2125,
     "status": "ok",
     "timestamp": 1745302338850,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "LfaUrYdSd_h5",
    "outputId": "a6ea5fdd-f054-4d65-84ea-8415ae071f79"
   },
   "outputs": [],
   "source": [
    "from obspy.core import Stream\n",
    "\n",
    "def main_autocorrelation(dstream, freq_width=0.5, npts=60*40, taper_width=0.5, pws_order=1,\n",
    "                         filter_kw={'freqmin':1.0, 'freqmax':5.0, 'corners':4, 'zerophase':True}):\n",
    "    \"\"\"\n",
    "    Compute the autocorrelation function for the given stream.\n",
    "\n",
    "    dstream: input stream of data\n",
    "    freq_width: spectral whitening width\n",
    "    npts: number of points for the autocorrelation function\n",
    "    \"\"\"\n",
    "    ## Compute autocorrelograms for all downloaded data\n",
    "    auto_stream = Stream()\n",
    "    for tr in dstream.copy():\n",
    "        auto = compute_auto(tr, freq_width=freq_width, npts=npts, taper_width=taper_width,\n",
    "                            filter_kwargs=filter_kw)\n",
    "        auto_stream.append(auto)\n",
    "    ## Data linear stack (DLS)\n",
    "    dls = data_stack(auto_stream, 0)\n",
    "    ## Phase weighted stack (PWS) of order 1\n",
    "    pws = data_stack(auto_stream, pws_order)\n",
    "    ## Time vector for plotting\n",
    "    tvec = np.arange(dls.stats.npts) * dls.stats.delta\n",
    "    ## Pick the reflection time\n",
    "    t_2p = tvec[np.argmin(dls.data[tvec<6])]\n",
    "\n",
    "    ## Create a figure for plotting\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(7, 3.5), sharey=True, width_ratios=[1, 0.2, 0.2])\n",
    "    ## Plot individual autocorrelograms\n",
    "    for tr in auto_stream:\n",
    "        # normalize the individual autocorrelogram by its maximum value\n",
    "        data = tr.data / np.max(np.abs(tr.data))\n",
    "        dist = tr.stats.dist / 111.195\n",
    "        # plot the waveform using its trace index in the stream as the x-axis\n",
    "        ax[0].plot(data+dist, tvec, lw=0.5, color='black')\n",
    "        # fill the negative part of the waveform with gray color\n",
    "        ax[0].fill_betweenx(tvec, dist, data+dist, lw=0.5, color='gray', where=(data < 0))\n",
    "    ax[0].set(ylim=(6, 0), ylabel='Lapse time (s)', xlabel='Epicentral distance (°)',\n",
    "              title=f'{dstream[0].stats.station} | Individual autocorrelograms')\n",
    "    ax[0].set(xlim=(30, 95))\n",
    "    ## Plot the DLS\n",
    "    dls.data /= np.max(np.abs(dls.data))\n",
    "    ax[1].plot(dls.data, tvec, lw=0.5, color='black')\n",
    "    ax[1].plot(0, t_2p, marker=4, c='r', ms=8)\n",
    "    ax[1].text(0.25, t_2p, '%.2fs'%t_2p, color='r', va='center', fontsize='small')\n",
    "    ax[1].fill_betweenx(tvec, 0, dls.data, lw=0.5, color='gray', where=(dls.data < 0))\n",
    "    ax[1].set(xlim=(-1, 1), xlabel='Norm. Amp.', title='DLS')\n",
    "    ## Plot the PWS\n",
    "    pws.data /= np.max(np.abs(pws.data))\n",
    "    ax[2].plot(pws.data, tvec, lw=0.5, color='black')\n",
    "    ax[2].fill_betweenx(tvec, 0, pws.data, lw=0.5, color='gray', where=(pws.data < 0))\n",
    "    ax[2].set(xlim=(-1, 1), xlabel='Norm. Amp.', title='PWS')\n",
    "    ## Plot the grid\n",
    "    for x in ax: x.grid(lw=0.2, color='gray')\n",
    "    ## Adjust the layout\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "## evaluate the function to compute the autocorrelation\n",
    "main_autocorrelation(dstream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbqh1zOAd_h5"
   },
   "source": [
    "The delayed of the reflection peak of the ice-bedrock interface, $t_{2p}$, is about 1.5 seconds. If taking the average *P*-wave speed, $v_p$, in ice is about 3.9 km/s, the estimate ice thickness beneath this station is:\n",
    "$$\n",
    "H \\approx \\dfrac{t_{2p}v_p}{2} = 2.9\\; \\text{km}.\n",
    "$$\n",
    "\n",
    "In the practical excercises, we will look at the autocorrelograms along a line of seismic stations and compare with ice thickness obtained from other method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KofomchAd_h5"
   },
   "source": [
    "---\n",
    "## Challenge\n",
    "\n",
    "Now, let take a little challenge by applying the processing presented above for a new seismic station. You are welcome to pick any station listed in the Table 1 in this paper (Pham & Tkalcic, [2018](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2018JB016115)) and compare the P-wave reflection time with the published result.\n",
    "\n",
    "A couple of ideas to experiment with your new dataset:\n",
    "- Change the `minmagnitude=6.0`\n",
    "- Change the spectral whitening width `freq_width=0.5`\n",
    "\n",
    "Once finished, submit you result to the google sheet below. We will see the collective effort comparing the ice thickness estimated from passive seismology and radio-echo sounding (included in the Bedmap2 dataset)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 89042,
     "status": "ok",
     "timestamp": 1745302427891,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "GxwswpFed_h5",
    "outputId": "ecb4d235-259b-4894-8d6e-541947e01674"
   },
   "outputs": [],
   "source": [
    "## Enter a station code (in Antarctica) of your choise\n",
    "stacode0 = 'P061' # Please replace the station code of your choice\n",
    "\n",
    "## Reminder of default parameters / Experiment with different values for the best outcome\n",
    "freq_width = 0.5 # Lets experiment with different spectral whitening widths\n",
    "npts = 60*40\n",
    "taper_width = 0.5\n",
    "pws_order = 1\n",
    "filter_kw = {'freqmin':1.0, 'freqmax':5.0, 'corners':4, 'zerophase':True}\n",
    "minmagnitude = 6.0\n",
    "\n",
    "## find station and download vertical P-wave coda seismograms\n",
    "inv0 = iris.get_stations(network='_ANTARCTICA', station=stacode0, channel='?HZ', level='channel')\n",
    "# set bulk_size to None to download all available data of the station\n",
    "dstream0 = download_data(inv0, bulk_size=300, min_magnitude=minmagnitude)\n",
    "# autocorrelation processing\n",
    "main_autocorrelation(dstream0, freq_width=freq_width, npts=npts, taper_width=taper_width, pws_order=pws_order, filter_kw=filter_kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZXYNLUud_h5"
   },
   "source": [
    "Please enter your result to [Google Sheets](https://docs.google.com/spreadsheets/d/1SC8b5G2wKP6pjR94jK4HV0Kk0h-mKE1hHwPCxzKIfMc/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWc4gc38fb-r"
   },
   "source": [
    "---\n",
    "## Conclusions\n",
    "\n",
    "- Autocorrelation is a special form of a broader research topic in modern seismomology known as `seismic interferometry`.\n",
    "\n",
    "- This is relatively simple technique but proves to be effective in imaging pronounced but shallow seismic discontinuities such as the ice-bedrock interface beneath ice sheet.\n",
    "\n",
    "- Application of this technique to image sedimentary basins (in different places in Vietnam) is unexplored but promises some interesting outcome."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "emcee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

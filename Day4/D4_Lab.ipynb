{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teleseismic *P*-wave coda autocorrelation\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tsonpham/ObsSeis-VNU/blob/master/Day4/D4_Lab.ipynb)\n",
    "\n",
    "Thanh-Son Pham (ThanhSon.Pham@anu.edu.au) \n",
    "\n",
    "- October 2017: First release on [GitHub](https://github.com/tsonpham/PCodaAutocorrelation.git).\n",
    "- March 2025: Revision for Observational Seismology Workshop at VNU Hanoi University of Science.\n",
    "\n",
    "This notebook implements a lightweight version of the *P wave coda autocorrelation* method in the Python programing language. The method is demonstrated with seismic data acquired by station [YT ST01](https://ds.iris.edu/mda/YT/ST01/?starttime=2010-01-27T00:00:00&endtime=2012-12-31T23:59:59) operating from 2010 to 2012 over the ~3 km thick icesheet in West Antarctica.\n",
    "\n",
    "For more details, we refer to the following articles:\n",
    "- Phạm T.-S. and H. Tkalčić, *On the feasibility and use of teleseismic P-wave coda autocorrelation for mapping shallow seismic discontinuities*, J. Geophys. Res.: Solid Earth, 122, doi:10.1002/2017JB013975, **2017**.\n",
    "- Phạm T.-S. and H. Tkalčić, *Antarctic ice properties revealed from teleseismic P wave coda autocorrelation*, J. Geophys. Res.: Solid Earth, 123, doi:10.1029/2018JB016115, **2018**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this method, we autocorrelate seismic records containing the P-wave arrival and its intermediate coda to reveal reflection signals from subsurface discontinuity. As described below, we first use `spectral normalization` to adjust the frequency content of individual P-wave coda record. The autocorrelation is then calculated in the frequency domain by squaring the modified power spectrum. Autocorrelograms corresponding to multiple earthquakes are then summed up (i.e., stacked) to improve the signal to noise ratio.\n",
    "\n",
    "<!-- ![](https://agupubs.onlinelibrary.wiley.com/cms/asset/5fab2346-82da-41a5-a4b9-68965dee2cea/jgrb52088-fig-0004-m.jpg) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral normalization\n",
    "We use an adaptive weighting function to normalize the complex spectrum of the input waveform before computing the autocorrelation function. The spectral whitening is defined in a similar way to the running-absolute-mean normalization [Bensen et al., 2007] in the time domain as\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{s}_n = \\frac{s_n} {\\frac{1}{2N + 1} \\sum_{j=n-N}^{n+N} |s_j|}.\n",
    "\\end{equation}\n",
    "\n",
    "The number of averaging points $N$ in the denominator is alternatively refered as spectral whitening width $\\Delta W = 2 N \\Delta \\omega$, where $\\Delta \\omega$ is the discrete frequency step $\\Delta \\omega = 1/L$ and $L$ is the length of the input time trace. Tuning the whitening width $\\Delta W$ may change the performance of the method. We recommend to do experiments with this parameter to find an optimal value for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import fftconvolve, hilbert\n",
    "\n",
    "def smooth_avg(arr, winlen):\n",
    "    \"\"\"\n",
    "    Smoothing a time series by averaging its values in a moving window. This averaging \n",
    "    window operation can be done efficiently with an convolution in the frequency domain.\n",
    "    \"\"\"\n",
    "\n",
    "    window = np.ones(winlen) / (1.0 * winlen)\n",
    "    return fftconvolve(arr, window, 'same')\n",
    "\n",
    "def spectral_whitening(arr, delta, freq_width, npts=None, returnweight=False):\n",
    "    \"\"\"\n",
    "    Frequency whitening of a time series by balancing its spectrum by smoothed absolute\n",
    "    array of the spectrum.\n",
    "    \n",
    "    arr        : input seismogram\n",
    "    delta      : sampling time step of the input\n",
    "    freq_width : spectral whitening width\n",
    "    returnweith: return the averaging width or not\n",
    "    \"\"\"\n",
    "    npts = len(arr) if npts is None else npts\n",
    "    carr = np.fft.fftshift(np.fft.fft(arr, 2 * npts))\n",
    "\n",
    "    Nyfreq = 0.5 / delta\n",
    "    spec_step = Nyfreq / npts\n",
    "    if freq_width != None:\n",
    "        winlen = int(0.5 * freq_width / spec_step) * 2 + 1\n",
    "        weight = smooth_avg(np.abs(carr), winlen)\n",
    "        if any(weight < 1e-8): raise Exception('Zero division')\n",
    "        \n",
    "        carr /= weight\n",
    "        carr[weight<1e-6*np.max(weight)] = 0\n",
    "\n",
    "    if returnweight:\n",
    "        return carr[npts-1:2*npts], weight[npts-1:2*npts]\n",
    "    else:\n",
    "        return carr[npts-1:2*npts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation\n",
    "The autocorrelation of a time trace is the inverse transform of the whitend power spectrum. \n",
    "$$\n",
    "a_n(\\omega) = |\\hat{s}_n|^2\n",
    "$$\n",
    "The discrete spectrum of the autocorrelation in the frequemcy domain, $a_n(\\omega)$ is inverse Fourier transformed to the time domain.\n",
    "\n",
    "The only causal part of the autocorrelation trace is retained. We use a cosine taper to the central peak and bandpass in the frequency band 1-5 Hz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auto(tr, freq_width, npts=None):\n",
    "    \"\"\"\n",
    "    Computing the autocorrelation function of the whitened trace.\n",
    "    \n",
    "    tr: input data seismogram\n",
    "    freq_width: spectral whitening width\n",
    "    \"\"\"\n",
    "    npts = tr.stats.npts if npts is None else npts\n",
    "    ## whitening and autocorrelating\n",
    "    spectrum = spectral_whitening(tr.data, tr.stats.delta, freq_width, npts)\n",
    "    ## autocorrelating\n",
    "    tr.data = np.fft.irfft(np.abs(spectrum)**2)[0:npts]\n",
    "    \n",
    "    # post processing: tapering and filtering\n",
    "    taper_width = 0.5 / (npts * tr.stats.delta)\n",
    "    tr.taper(type='cosine', max_percentage=taper_width)\n",
    "    tr.filter('bandpass', freqmin=1.0, freqmax=5.0, corners=4, zerophase=True)\n",
    "    tr.taper(type='cosine', max_percentage=taper_width)\n",
    "\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase weighted stacking\n",
    "\n",
    "*Phase-weighted stacking* is a nonlinear stacking technique to sufficiently enhance coherent signals and suppress noise [Schimmel and Paulssen, 1997]. It uses the averaging phase of the input traces to weight the linear stack of the input.\n",
    "\n",
    "The analytical signals of a one-sided autocorrelogram $s_n$ is given by\n",
    "$$\n",
    "S_n(t) = s_n(t) + i H_n(t) = A_n(t) e^{i\\Phi_n(t)}\n",
    "$$\n",
    "where $H_n(t)$ is the Hilbert transform of the original trace $s_n(t)$. And, $A_n(t)$, $\\Phi_n(t)$ are the amplitude and phase components of the analytic signal. The phase-weighted stack of $N$ input traces is defined as\n",
    "$$\n",
    "g(t) = \\frac{1}{N} \\sum_{n=1}^N s_n(t) \\; \\left|\\frac{1}{N}\\sum_{n=1}^N e^{i\\Phi_n(t)}\\right|^\\eta.\n",
    "$$\n",
    "The order $\\eta$ is used to adjust the importance of the phase weighting factor. When $\\eta = 0$, the 0th order phase-weighted stack becomes a linear stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.core import Trace\n",
    "\n",
    "def data_stack(stream, order):\n",
    "    \"\"\"\n",
    "    Phase weighted stacking\n",
    "    \n",
    "    stream: input stream of one-sided autocorrelograms\n",
    "    order : order of the weight phase\n",
    "    \"\"\"\n",
    "    stack = 0\n",
    "    phase = 0j\n",
    "    \n",
    "    if order == 0:\n",
    "        stack = np.sum([acorr.data for acorr in stream], axis=0)\n",
    "        return Trace(header={'npts': len(stack), 'delta': stream[0].stats.delta}, data=stack)\n",
    "    \n",
    "    for acorr in stream:\n",
    "        stack += acorr.data\n",
    "\n",
    "        ## calculate phase\n",
    "        asig = hilbert(acorr.data)\n",
    "        phase += asig / np.abs(asig)\n",
    "\n",
    "    stack /= len(stream)\n",
    "    weight = np.abs(phase / len(stream))\n",
    "\n",
    "    return Trace(header={'npts': len(stack), 'delta': stream[0].stats.delta}, data=stack * weight**order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application for pilot station ST01 (West Antarctica)\n",
    "\n",
    "This section contains step-by-step guide to acquire seismic data from remote server, perform processing, visualise data, and make initial interpretation of the results in the geological context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "First, we query the geographical station of station ST01 from IRIS Data Management center using an `obspy fdsn client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## definte the IRIS client\n",
    "iris = Client('IRIS')\n",
    "\n",
    "## define the network and station code of interest\n",
    "netcode = 'YT'\n",
    "stacode = 'ST01'\n",
    "\n",
    "## get station information\n",
    "inv = iris.get_stations(network=netcode, station=stacode, level='station')\n",
    "\n",
    "## get the station coordinates\n",
    "stla = inv[0][0].latitude\n",
    "stlo = inv[0][0].longitude\n",
    "\n",
    "## plot the station location on map for sanity check\n",
    "m = Basemap(projection='spstere', boundinglat=-65, lon_0=180, resolution='l')\n",
    "m.drawcoastlines()\n",
    "m.plot(stlo, stla, 'r*', markersize=10, latlon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, search for all catalogued earthquakes of magnitude 6.0+, between distances from 30 to 95$^\\circ$ from the seismic station of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the event metadata from the IRIS server\n",
    "cat = iris.get_events(starttime=inv[0][0].start_date, endtime=inv[0][0].end_date, \n",
    "    latitude=stla, longitude=stlo, minradius=30.0, maxradius=95.0, minmagnitude=6.0)\n",
    "\n",
    "## plot the event location on a global map\n",
    "m = Basemap(projection='robin', lon_0=0, resolution='c')\n",
    "m.drawcoastlines(linewidth=.75)\n",
    "## plot event location by red stars\n",
    "m.scatter([_.preferred_origin().longitude for _ in cat], [_.preferred_origin().latitude for _ in cat], \n",
    "    latlon=True, s=10, c='r', marker='*')\n",
    "## plot the station location by blue triangle\n",
    "m.plot(stlo, stla, 'b^', markersize=10, latlon=True)\n",
    "\n",
    "## plot the tissots showing the distance from the station\n",
    "_lats = np.linspace(-90, 90, 100)\n",
    "_lons = np.linspace(-180, 180, 100)\n",
    "lons, lats = np.meshgrid(_lons, _lats)\n",
    "from obspy.geodetics import locations2degrees\n",
    "gcarc = locations2degrees(lats, lons, stla, stlo)\n",
    "cs = m.contour(lons, lats, gcarc, levels=[30, 95], colors='k', latlon=True)\n",
    "# labels the countour inline with some custom format\n",
    "plt.clabel(cs, fmt='%2.0f$^\\circ$', inline=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, waveform data corresponding to with seismic station codes and event catalog are requested. To do so, we need to first predict the travel time of *P* from sources to the receivers. Then request 60-second long data segments starting from 10 second before the predicted time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.taup import TauPyModel\n",
    "from obspy.geodetics import gps2dist_azimuth\n",
    "from obspy.clients.fdsn.client import FDSNNoDataException\n",
    "taup_model = TauPyModel(model='ak135')\n",
    "\n",
    "bulk = []\n",
    "for event in cat:\n",
    "    origin = event.preferred_origin()\n",
    "    ## calculate the P-wave arrival time from the event to the station\n",
    "    p_arv = taup_model.get_travel_times_geo(origin.depth/1e3, origin.latitude, \n",
    "                origin.longitude, stla, stlo, phase_list=['P'])[0]\n",
    "    p_arv_time = origin.time + p_arv.time\n",
    "    ## enclose the calculated information into list\n",
    "    bulk.append((netcode, stacode, '', 'BHZ,HHZ', p_arv_time-10, p_arv_time+50))\n",
    "    \n",
    "## get the waveform data\n",
    "dstream = iris.get_waveforms_bulk(bulk, threaded=True, minimumlength=55)\n",
    "\n",
    "## resample the data to 40 Hz (if necessary) and remove the linear data trend\n",
    "dstream.resample(40.0)\n",
    "dstream.detrend('linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelograms are computed for individual seismograms and then stacked to improve signal to noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.core import Stream\n",
    "\n",
    "## Compute autocorrelograms for all downlaoded data\n",
    "auto_stream = Stream()\n",
    "for tr in dstream.copy():\n",
    "    auto = compute_auto(tr, freq_width=0.5, npts=60*40) # 60 seconds at 40 Hz\n",
    "    auto_stream.append(auto)\n",
    "## Data linear stack (DLS)\n",
    "dls = data_stack(auto_stream, order=0)\n",
    "## Phase weighted stack (PWS) of order 1\n",
    "pws = data_stack(auto_stream, order=1)\n",
    "## Time vector for plotting\n",
    "tvec = np.arange(dls.stats.npts) * dls.stats.delta\n",
    "## Pick the reflection time\n",
    "t_2p = tvec[np.argmin(dls.data[tvec<6])]\n",
    "\n",
    "## Create a figure for plotting\n",
    "fig, ax = plt.subplots(1, 3, figsize=(7, 3.5), sharey=True, width_ratios=[1, 0.2, 0.2])\n",
    "## Plot individual autocorrelograms\n",
    "for i, auto in enumerate(auto_stream):\n",
    "    # normalize the individual autocorrelogram by its maximum value\n",
    "    data = auto.data / np.max(np.abs(auto.data))\n",
    "    # plot the waveform using its trace index in the stream as the x-axis\n",
    "    ax[0].plot(data + i, tvec, lw=0.5, color='black')\n",
    "    # fill the negative part of the waveform with gray color\n",
    "    ax[0].fill_betweenx(tvec, i, data + i, lw=0.5, color='gray', where=(data < 0))\n",
    "ax[0].set(ylim=(6, 0), ylabel='Lapse time (s)', xlabel='Trace index', title='Individual autocorrelograms')\n",
    "ax[0].set(xlim=(-1, len(auto_stream)))\n",
    "## Plot the DLS\n",
    "dls.data /= np.max(np.abs(dls.data))\n",
    "ax[1].plot(dls.data, tvec, lw=0.5, color='black')\n",
    "ax[1].plot(0, t_2p, marker=4, c='r', ms=8)\n",
    "ax[1].text(0.25, t_2p, '%.2fs'%t_2p, color='r', va='center', fontsize='small')\n",
    "ax[1].fill_betweenx(tvec, 0, dls.data, lw=0.5, color='gray', where=(dls.data < 0))\n",
    "ax[1].set(xlim=(-1, 1), xlabel='Norm. Amp.', title='DLS')\n",
    "## Plot the PWS\n",
    "pws.data /= np.max(np.abs(pws.data))\n",
    "ax[2].plot(pws.data, tvec, lw=0.5, color='black')\n",
    "ax[2].fill_betweenx(tvec, 0, pws.data, lw=0.5, color='gray', where=(pws.data < 0))\n",
    "ax[2].set(xlim=(-1, 1), xlabel='Norm. Amp.', title='PWS')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The delayed of the reflection peak of the ice-bedrock interface, $t_{2p}$, is about 1.5 seconds. If taking the average *P*-wave speed, $v_p$, in ice is about 3.9 km/s, the estimate ice thickness beneath station ST01 is. \n",
    "$$\n",
    "H \\approx \\dfrac{t_{2p}v_p}{2} = 2.9\\; \\text{km}.\n",
    "$$\n",
    "\n",
    "In the practical excercises, we will look at the autocorrelograms along a "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emcee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

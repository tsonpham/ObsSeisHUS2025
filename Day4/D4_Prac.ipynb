{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U_I1-_8eatA"
   },
   "source": [
    "# Single-event global correlogram\n",
    "\n",
    "[![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day4/D4_Prac.ipynb)\n",
    "\n",
    "Prepared by Thanh-Son Pham (thanhson.pham@anu.edu.au), April 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Yal01EJNC2a"
   },
   "source": [
    "---\n",
    "## What we do in this notebook\n",
    "\n",
    "Global correlation wavefield is an emerging concept in global seismology. It refers to the mathematical manifestication of the seismic wavefield. Here the seemingly noisy records (3-10 hours) after large earthquakes are turned into clear signals that are sensitive to the deep Earth's interiors.\n",
    "\n",
    "- Learn about mathematical cross-correlation of digital seismographs\n",
    "- Step-by-step processing to calculate seismic cross-correlation functions at the global scale\n",
    "- First step to handle large amount of seismic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrvqX8s2eatG"
   },
   "outputs": [],
   "source": [
    "# Environemtal setup (uncomment if running in colab)\n",
    "\n",
    "# !pip install obspy numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuB4de9e4UlB"
   },
   "outputs": [],
   "source": [
    "#@title Setting notebook resolution\n",
    "#@markdown Run this cell for better figure resolution\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"savefig.dpi\"] = 100\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "rcParams[\"font.size\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iN0W5KbaeatF"
   },
   "source": [
    "---\n",
    "## Data preparation and preprocessing\n",
    "\n",
    "This notebook is designed to be inclusive starting from fetching raw seismic waveforms from IRIS Data Management Center. The next cell defines three functions to\n",
    "1. acquire event metadata from the International Seismological Center (ISC) earthquake bulletin using unique event ID,\n",
    "2. fetch raw seismic data of the vertical component in a designed time window with respect to the event origin,\n",
    "3. apply simple data processing procedure to the fetched data.\n",
    "\n",
    "The functions make use of several `Obspy` functionalities introduced in previous modules. The cell must be excecuted, but its content can be skipped as matter of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GmmSrCteatH"
   },
   "outputs": [],
   "source": [
    "#@title Definition of helper functions\n",
    "#@markdown Helper function to download and process seismic data from IRIS\n",
    "\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "from pathlib import Path\n",
    "from obspy import read_events, read_inventory, read\n",
    "from obspy.geodetics import locations2degrees\n",
    "import pickle\n",
    "\n",
    "def get_event(isc_eventid):\n",
    "    \"\"\"\n",
    "    Download the event information from ISC and return the event object.\n",
    "    Parameters\n",
    "    ----------\n",
    "    isc_eventid : str\n",
    "        The ISC event ID of the event to download.\n",
    "    Returns\n",
    "    -------\n",
    "    event : obspy.core.event.Event\n",
    "        The event object containing the event information.\n",
    "    \"\"\"\n",
    "    # file name for the event information\n",
    "    fname = Path(f'ISC.{isc_eventid}.xml')\n",
    "    # Download the event information from ISC if not already downloaded\n",
    "    if not fname.exists():\n",
    "        event = Client('ISC').get_events(eventid=isc_eventid, filename=fname)\n",
    "    # Read the event information from the file\n",
    "    event = read_events(fname)[0]\n",
    "    return event\n",
    "\n",
    "def get_waveform_data(event_origin, network='_GSN', channel='LHZ', window_start=10e3, window_end=35e3,\n",
    "        loc_priorities = ['', '00', '10'], with_inv=False):\n",
    "    '''\n",
    "    Download the waveform data from IRIS for a given event and return the stream object.\n",
    "    Parameters\n",
    "    ----------\n",
    "    event_origin : obspy.core.event.Origin\n",
    "        The event origin object containing the event information.\n",
    "    network : str\n",
    "        The network code to download the waveform data from.\n",
    "    channel : str\n",
    "        The channel code to download the waveform data from.\n",
    "    window_start : float\n",
    "        The start time of the window to download the waveform data from.\n",
    "    window_end : float\n",
    "        The end time of the window to download the waveform data from.\n",
    "    loc_priorities : list of str\n",
    "        The location codes to download the waveform data from.\n",
    "    Returns\n",
    "    -------\n",
    "    dstream : obspy.core.stream.Stream\n",
    "        The stream object containing the waveform data.\n",
    "    '''\n",
    "    eventtime = event_origin.time\n",
    "    evla = event_origin.latitude\n",
    "    evlo = event_origin.longitude\n",
    "    # Fearch for all stations in the _GNS network recording in the time window\n",
    "    inv_fname = Path(network+eventtime.strftime('.%Y%m%d.staxml'))\n",
    "    # Download the station information from IRIS unless already downloaded\n",
    "    if not inv_fname.exists():\n",
    "        Client().get_stations(network=network, channel=channel, starttime=eventtime,\n",
    "                              endtime=eventtime+24*3600, level='channel', filename=inv_fname)\n",
    "    # Read the station information from the file\n",
    "    inv = read_inventory(inv_fname)\n",
    "\n",
    "    # For each station, add a line to the bulk request depending on their location code\n",
    "    # and the location code with the highest priority\n",
    "    # The location codes are sorted by priority\n",
    "    bulk = []\n",
    "    for net in inv:\n",
    "        for sta in net:\n",
    "            loc_codes = [cha.location_code for cha in sta.channels]\n",
    "            if len(set(loc_priorities) & set(loc_codes)) == 0: continue\n",
    "            # get the location code with the highest priority\n",
    "            for loccode in loc_priorities:\n",
    "                if loccode in loc_codes: break\n",
    "            # add the bulk entry\n",
    "            bulk.append((net.code, sta.code, loccode, channel,\n",
    "                # the time window is padded by 2,500 seconds at both ends to for quality assurance\n",
    "                eventtime+window_start-2.5e3, eventtime+window_end+2.5e3))\n",
    "\n",
    "    # Download the waveforms from IRIS if not already downloaded\n",
    "    pkl_fname = Path(network+eventtime.strftime('.%Y%m%d.pkl'))\n",
    "    if not pkl_fname.exists():\n",
    "        print (f'Requesting waveforms for {len(bulk)} individual {channel} channels ...')\n",
    "        dstream = Client().get_waveforms_bulk(bulk, longestonly=True, minimumlength=7*3600)\n",
    "        # Append metadata to header\n",
    "        for tr in dstream:\n",
    "            tmp = inv.select(network=tr.stats.network, station=tr.stats.station)[0][0]\n",
    "            tr.stats.update({'stla': tmp.latitude, 'stlo': tmp.longitude,\n",
    "                'distance': locations2degrees(evla, evlo, tmp.latitude, tmp.longitude)*111.195e3})\n",
    "        with open(pkl_fname, 'wb') as f: pickle.dump(dstream, f)\n",
    "    else:\n",
    "        # print ('Reading waveforms ...')\n",
    "        with open(pkl_fname, 'rb') as f: dstream = pickle.load(f)\n",
    "    if with_inv:\n",
    "        return dstream, inv\n",
    "    else:\n",
    "        return dstream\n",
    "\n",
    "def data_processing(dstream, channel, evtime, window_start, window_end):\n",
    "    '''\n",
    "    Process the waveform data by resampling, detrending, tapering, and trimming.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dstream : obspy.core.stream.Stream\n",
    "        The stream object containing the waveform data.\n",
    "    channel : str\n",
    "        The channel code to process.\n",
    "    evtime : obspy.core.UTCDateTime\n",
    "        The event origin time.\n",
    "    window_start : float\n",
    "        The start time of the window to process the waveform data from.\n",
    "    window_end : float\n",
    "        The end time of the window to process the waveform data from.\n",
    "    Returns\n",
    "    -------\n",
    "    dstream1 : obspy.core.stream.Stream\n",
    "        The processed stream object containing the waveform data.\n",
    "    '''\n",
    "    # Create a copy of the stream object\n",
    "    dstream1 = dstream.copy()\n",
    "    # Downsample the data to 1 Hz if the channel is not LHZ\n",
    "    if channel != 'LHZ': dstream1.resample(1.0)\n",
    "    # Remove the linear trend and taper the data with a cosine taper\n",
    "    dstream1.detrend('linear')\n",
    "    dstream1.taper(max_percentage=0.01, type='cosine')\n",
    "    # Trim the data to the time window and pad with zeros\n",
    "    dstream1.trim(evtime+window_start, evtime+window_end, pad=True, fill_value=0)\n",
    "    return dstream1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyotSd_GNC2n"
   },
   "source": [
    "We call the helper function in the next cell to download `LHZ` component, sampled at 1 Hz, from all stations belonging to the [*Global Seismograph Network*](https://www.usgs.gov/programs/earthquake-hazards/gsn-global-seismographic-network). These state-of-the-art seismometers continuously monitor the global ground motion with data telecommunicated to data centers in real time.\n",
    "\n",
    "\"*The Global Seismographic Network is a permanent digital network of state-of-the-art seismological and geophysical sensors connected by a telecommunications network, serving as a multi-use scientific facility and societal resource for monitoring, research, and education.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "executionInfo": {
     "elapsed": 49514,
     "status": "ok",
     "timestamp": 1744300838327,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "Ym_OxgKoNC2r",
    "outputId": "6ab6336e-7e02-421c-9913-6b83f7c1415f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Specify the event ID, network, channel, and time window\n",
    "isc_eventid = '611831635'\n",
    "network = '_GSN'\n",
    "channel = 'LHZ'\n",
    "window_start = 10e3\n",
    "window_end = 35e3\n",
    "sampling_rate = 1.0 # Hz by default because we are using LHZ\n",
    "npts = int((window_end-window_start)/sampling_rate) # number of samples in the time window\n",
    "\n",
    "print (f'Preparing data for Event ID: {isc_eventid}')\n",
    "print (f'   Network: {network}')\n",
    "print (f'   Channel: {channel} sampled at {sampling_rate} Hz')\n",
    "print (f'   Time window from origin to {window_end} seconds')\n",
    "\n",
    "# fetch event metadata using ISC event ID,\n",
    "event = get_event(isc_eventid)\n",
    "# download the waveforms from IRIS unless they existed locally\n",
    "dstream, inv = get_waveform_data(event.preferred_origin(), network, channel, 0, window_end, with_inv=True)\n",
    "print (f'   Number of retrieved waveforms {len(dstream)}')\n",
    "\n",
    "# plot waveform gather for visual inspection\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for tr in dstream: \n",
    "    tr.data = tr.data / np.max(np.abs(tr.data)) # normalize the data\n",
    "dstream.plot(type='section', orientation='horizontal', color='black', fig=fig, starttime=event.preferred_origin().time, scale=20)\n",
    "fig.gca().set_title(f'Event ID: {isc_eventid} - {event.preferred_origin().time} UTC', fontsize=12)\n",
    "fig.gca().axvline(window_start, color='red', linestyle='-', lw=1)\n",
    "fig.gca().axvline(window_end, color='red', linestyle='-', lw=1)\n",
    "fig.gca().annotate('', xy=(window_start, 16e3), xytext=(window_end, 16e3), arrowprops=dict(arrowstyle='<->', lw=1, color='red'))\n",
    "fig.gca().annotate('Coda window', xy=(.5*(window_start+window_end), 16.3e3), color='red', fontsize=12, fontweight='bold', ha='center')\n",
    "fig.gca().set(ylim=(0, 180*111.195))\n",
    "\n",
    "# plot the station map distribution\n",
    "inv.plot(label=False, size=20)\n",
    "plt.show()\n",
    "\n",
    "# do some simple data processing: trim the waveform at desired time window and resample 1 Hz\n",
    "dstream = data_processing(dstream, channel, event.preferred_origin().time, window_start, window_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIfPFBa4DQo6"
   },
   "source": [
    "First, we use the unique event identifier [611831635](https://isc.ac.uk/cgi-bin/web-db-run?event_id=611831635&out_format=ISF2&request=COMPREHENSIVE) to get an event metadata from the [ISC](https://isc.ac.uk/iscbulletin/search/bulletin/) bulletin.\n",
    "\n",
    "The virtual network code `_GSN` (see complete list [here](https://earthquake.usgs.gov/monitoring/operations/network.php?virtual_network=GSN)) is used to include qualifying stations being parts of in several regional networks.\n",
    "\n",
    "Vertical, long period component seismograms `LHZ`, by default sampled at 1 `Hz`, are suitable to balance between computational expense and spectral resolution.\n",
    "\n",
    "Phạm et al. (2018) used the waveform window from 10,000 to 35,000 seconds (roughly 3 to 10 hours) after large earthquakes to compute the global correlograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70t7iggueatJ"
   },
   "source": [
    "---\n",
    "## Single station processing\n",
    "\n",
    "It has been demonstrated above that the cross-correlation function can be computed efficiently in the spectral domain. This is especially important when computing CCF for an array of many stations using long seismic records.\n",
    "\n",
    "In practice, temporal and spectral normalizations are crucial in extracting clear signals in the cross-correlogram.\n",
    "\n",
    "- Because the level of seismic energy generally decays with time, *temporal normalization* is needed to balance the contribution of high amplitude data early in the coda record.\n",
    "\n",
    "- Spectral normalization balances the contribution of signals at different frequency in the interested frequency band. Generally long-period signals are more energetic than shorter period signals because the former ones are dissipated quicklier when propagating through the Earth.\n",
    "\n",
    "Following defines the temporal and spectral normalization operators using the same running absolute mean weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncKQuywDeatK"
   },
   "source": [
    "### Temporal normalization\n",
    "\n",
    "We use an adaptive weighting function to to balance amplitude-decaying amplitudes. Phạm et al. (2018) used the running-absolute-mean [Bensen et al., 2007] as for the weight in the time domain as\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{x}_n = \\frac{x_n} {\\frac{1}{2N + 1} \\sum_{j=n-N}^{n+N} |x_j|}.\n",
    "\\end{equation}\n",
    "\n",
    "The number of averaging points $N$ in the denominator is alternatively refered as the temporal normalization width $\\Delta W = 2 N \\Delta T$, where $\\Delta T$ is the discrete sampling interval of the input seismogram.\n",
    "\n",
    "Phạm et al. (2018) empirically used $2N\\Delta T = 128$ seconds in their work. They also pre-filtered the seismogram before calculating the weight. However, the discussion on the role of the pre-filtering step is beyond the scope of this notebook. We refer to the orignial article and there-in reference to Bensen et al. (2007) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dm31uGojeatK"
   },
   "outputs": [],
   "source": [
    "from obspy import Trace, Stream\n",
    "\n",
    "def running_absolute_mean_normalization(input_tr, width, filter_kw=None, return_weight=False):\n",
    "    \"\"\"\n",
    "    Apply running absolute mean normalization to a waveform/spectral trace.\n",
    "    :param trace: ObsPy Trace object\n",
    "    :param fband: frequency band for the filter / None if no pre-filtering is applied to the weight\n",
    "    :param width: width of the running absolute mean normalization window in seconds or Hz depending on the input_tr\n",
    "    :return: output_tr trace\n",
    "    \"\"\"\n",
    "    if type(input_tr) is not Trace:\n",
    "        raise TypeError('Input trace is not an ObsPy Trace object.')\n",
    "    # pre-filter the weight trace if frequency filter band is given\n",
    "    weight_tr = input_tr.copy()\n",
    "    output_tr = input_tr.copy()\n",
    "    if type(input_tr.data[0]) is np.complex128 or type(input_tr.data[0]) is np.complex64:\n",
    "        delta = input_tr.stats.delta_f\n",
    "    else:\n",
    "        delta = input_tr.stats.delta\n",
    "    # if pre-filtering is indicated, apply the filter to the weight trace\n",
    "    if filter_kw is not None: weight_tr.filter(**filter_kw)\n",
    "    # calculate the normalizing weight by running absolute mean\n",
    "    winlen = 2 * int(0.5 * width / delta) + 1\n",
    "    avg_mask = np.ones(winlen) / winlen\n",
    "    weight = np.convolve(np.abs(weight_tr.data), avg_mask, 'same')\n",
    "    # divide the orignal data by the smoothed weight\n",
    "    mask = (weight > 1e-8*np.max(weight))\n",
    "    output_tr.data[mask] = input_tr.data[mask] / weight[mask]\n",
    "    output_tr.data[np.logical_not(mask)] = 0\n",
    "    # return the output_tr trace\n",
    "    if return_weight:\n",
    "        return output_tr, weight\n",
    "    else:\n",
    "        return output_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNPTRiFmNC20"
   },
   "source": [
    "Let's demonstrate the running absolute mean temporal normalization on a real coda seismogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1744300867342,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "Yd3G6xV8NC20",
    "outputId": "9fdb6225-23ee-4ef4-9b75-84ff34846217"
   },
   "outputs": [],
   "source": [
    "# Temporal normalization parameters\n",
    "temp_width = 128 #seconds\n",
    "filter_kw = dict(type='bandpass', freqmin=0.02, freqmax=0.067, corners=4, zerophase=True) # bandpass filter between 15 to 50 seconds\n",
    "\n",
    "# Seismogram before temporal normalization\n",
    "input_tr = dstream[0].copy() # an example trace\n",
    "orig = input_tr.copy().filter(**filter_kw)\n",
    "\n",
    "# Seismogram after temporal normalization\n",
    "output_tr, weight = running_absolute_mean_normalization(input_tr, temp_width, filter_kw, True)\n",
    "norm = output_tr.copy().filter(**filter_kw)\n",
    "\n",
    "# Plot the comparison\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "ax[0].plot(orig.times(), orig.data, c='k', label='Original')\n",
    "ax[0].plot(norm.times(), weight, c='gray', label='Running weight')\n",
    "ax[0].set(ylabel='Orig. amp.', title='Orignal waveforms and running absolute mean weight')\n",
    "ax[0].legend()\n",
    "ax[1].plot(norm.times(), norm.data, c='k', label='Normalized')\n",
    "ax[1].set(ylabel='Norm. amp.', title='Normalized = Orignal / Running weight', xlabel='Time (s)')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0ag9Z56eatL"
   },
   "source": [
    "### Spectral normalization\n",
    "\n",
    "Now, the temporally normalized seismograms are Fourier transformed to the frequency domain in prepration for the fast computation of CCFs.\n",
    "\n",
    "A similar running-absolute-mean normalization is applied to the spectral domain to balance the contribution of signals at different periods to the final CCF. The spectral normalization is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{s}_n = \\frac{s_n} {\\frac{1}{2N + 1} \\sum_{j=n-N}^{n+N} |s_j|}.\n",
    "\\end{equation}\n",
    "\n",
    "$s_n$ is the spectrogram of the input seismogram. The number of averaging points $N$ in the denominator is alternatively refered as the spectral normalization width $\\Delta W = 2 N \\Delta \\omega$, where $\\Delta \\omega$ is the discrete frequency step $\\Delta \\omega = 1/L$ and $L$ is the length of the input time trace.\n",
    "\n",
    "Phạm et al. (2018) empirically used $2N\\Delta\\omega = 2\\times 10^{-3}$ Hz as the spectral normalization width in their work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzPNdSnKeatL"
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import next_fast_len\n",
    "def spectral_normalization(input_tr, npts, ram_width, return_weight=False):\n",
    "    \"\"\"\n",
    "    Apply spectral normalization to a waveform trace.\n",
    "    :param input_tr: ObsPy Trace object\n",
    "    :param npts: number of points for FFT\n",
    "    :param ram_width: width of the running absolute mean normalization window in seconds\n",
    "    :return: output_tr trace\n",
    "    \"\"\"\n",
    "    # get the number of points for FFT using tensorflow fft module\n",
    "    fft_npts = next_fast_len(2*npts)\n",
    "    # real-to-complex FFT by numpy\n",
    "    spec_data = np.fft.rfft(input_tr.data, fft_npts)\n",
    "    # create a spectral trace\n",
    "    spec_tr = Trace(data=spec_data, header=input_tr.stats)\n",
    "    spec_tr.stats.update({'npts':len(spec_data), 'delta_f':.5/((len(spec_data)-1)*input_tr.stats.delta)})\n",
    "    # apply running absolute mean normalization to the spectral trace\n",
    "    if ram_width is not None:\n",
    "        return running_absolute_mean_normalization(spec_tr, ram_width, return_weight=return_weight)\n",
    "    else:\n",
    "        return spec_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iM11DfwNC24"
   },
   "source": [
    "Let's demonstrate the effect of the spectral normalization function on the already temporally normalised seismogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 2251,
     "status": "ok",
     "timestamp": 1744300876438,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "jO33fGddNC24",
    "outputId": "a8da1ffa-dd64-4210-933b-5dff208e5107"
   },
   "outputs": [],
   "source": [
    "# Spectral normalization parameters\n",
    "spec_width = 2e-3 # Hz\n",
    "\n",
    "# apply spectral normalization to the data\n",
    "spec_tr, weight = spectral_normalization(output_tr, npts, spec_width, True)\n",
    "\n",
    "# when spec_width is None, the plain spectrum of the input trace is returned for comparison\n",
    "spec_tr0 = spectral_normalization(output_tr, npts, None)\n",
    "\n",
    "# Plot the comparison\n",
    "fig, ax = plt.subplots()\n",
    "freqs = np.arange(spec_tr.stats.npts) * spec_tr.stats.delta_f\n",
    "ax.semilogy(freqs, np.abs(spec_tr.data), color='k', label='Normalized')\n",
    "ax.semilogy(freqs, np.abs(spec_tr0.data), color='gray', label='Original')\n",
    "ax.semilogy(freqs, np.abs(weight), color='r', label='Running weight')\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "ax.set_ylabel('Spec. Amp.')\n",
    "ax.set_xlim([0, 0.1])\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NSXpyjpeatL"
   },
   "source": [
    "### Single station processing: temporal and spectral normalizations\n",
    "\n",
    "Now we concatenate the temporal and spectral normalization operators together and apply them to all seismograms in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKUqKt39NC26"
   },
   "outputs": [],
   "source": [
    "def single_station_processing(dstream):\n",
    "    spec_st = Stream()\n",
    "    for data_tr in dstream:\n",
    "        temp_tr = running_absolute_mean_normalization(data_tr, temp_width, filter_kw)\n",
    "        spec_tr = spectral_normalization(temp_tr, npts, spec_width)\n",
    "        spec_st.append(spec_tr)\n",
    "    return spec_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1500,
     "status": "ok",
     "timestamp": 1744300902365,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "RkxdkzcmeatM",
    "outputId": "c14a481b-73f4-4cca-c46d-56a31f7e1d52"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print ('Parameterisation used in the single-station processing:')\n",
    "print (f' Waveform channel: {channel}')\n",
    "print (f'       Event time: {event.preferred_origin().time}')\n",
    "print (f'       Window start: {event.preferred_origin().time+window_start}')\n",
    "print (f'       Window end: {event.preferred_origin().time+window_end}')\n",
    "print (f'       Sampling rate: {sampling_rate} Hz')\n",
    "print (f' Temporal normalization: running absolute mean normalization')\n",
    "print (f'       Temporal width: {temp_width} seconds')\n",
    "print (f'       Weight filtering band: {filter_kw[\"freqmin\"]} to {filter_kw[\"freqmax\"]} Hz')\n",
    "print (f' Spectral normalization: running absolute mean normalization')\n",
    "print (f'       Spectral width: {spec_width} Hz')\n",
    "print (f'       Number of points: {npts}')\n",
    "\n",
    "start = time.time()\n",
    "spec_st = single_station_processing(dstream)\n",
    "print (f'\\nSingle processing of {len(dstream)} traces took {time.time()-start:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM2CKAWDeatM"
   },
   "source": [
    "---\n",
    "## Cross-correlation of all station pairs\n",
    "\n",
    "We use the spectral data obtained from this step to perform cross-correlation in the frequency domain. The first task is to multiply one spectra to the conjugate of the other station. Secondly, based on the interstation distance between the two stations, add them to the bin corresponding to their distance.\n",
    "\n",
    "However, the practical challenge is there are a large number of spectral multiplications to excecute. As the number of operations is $O (N^2)$. The main rationale of the implementation in the cell below is to speed up that process using multithreading and vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JFgAhEYeatM"
   },
   "outputs": [],
   "source": [
    "from obspy.geodetics import locations2degrees\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "def xcorr_stack(spec_st, dist_bins, max_tlag, ncores=2, fname=None):\n",
    "    '''\n",
    "    Cross-correlation stacking of the spectral traces in frequency domain\n",
    "    Parameters\n",
    "    ----------\n",
    "    spec_st : obspy.core.stream.Stream\n",
    "        The stream object containing the spectral traces.\n",
    "    dist_bins : list of float\n",
    "        The distance bins to stack the cross-correlation function.\n",
    "    max_tlag : float\n",
    "        The maximum time lag to stack the cross-correlation function.\n",
    "    ncores : int or None\n",
    "        The number of cores to use for parallel processing. If None, use all available cores.\n",
    "    fname : str or None\n",
    "        The file name to save the output. If None, do not save the output.\n",
    "    Returns\n",
    "    -------\n",
    "    output : dict\n",
    "        The output dictionary containing the cross-correlation function and other information.\n",
    "    '''\n",
    "    stlats = [tr.stats.stla for tr in spec_st]\n",
    "    stlons = [tr.stats.stlo for tr in spec_st]\n",
    "    spec_data = np.array([tr.data for tr in spec_st])\n",
    "    # number of traces and frequency bins\n",
    "    ntraces, nspec = spec_data.shape\n",
    "    # number of cross-correlation distance bins\n",
    "    nbins = len(dist_bins) - 1\n",
    "    # global variables\n",
    "    spec_xcorr_stack = np.zeros([nbins, nspec], dtype=np.complex128)\n",
    "    bin_count = np.zeros(nbins, dtype=np.float32)\n",
    "\n",
    "    # calculate all inter-receiver distances\n",
    "    mlats, mlons = np.meshgrid(stlats, stlons)\n",
    "    cc_gcarc = locations2degrees(mlats, mlons, mlats.T, mlons.T)\n",
    "    # determine the bin index for each inter-receiver distance\n",
    "    cc_inds = np.digitize(cc_gcarc, dist_bins) - 1\n",
    "\n",
    "    # Create a lock for each bin\n",
    "    # Note: the multiple locks for each bin is recommended to avoid congested\n",
    "    # memory access when multiple threads are trying to access the same bin.\n",
    "    locks = [threading.Lock() for _ in range(nbins)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(ncores) as executor:\n",
    "        # Local cross-correlation function to excecuted in multiple threads\n",
    "        def cross_correlation(s1):\n",
    "            # Cross-correlation function in frequency domain\n",
    "            spec_xcorr = spec_data[s1] * np.conj(spec_data[s1:])\n",
    "            for _, ind in enumerate(cc_inds[s1, s1:]):\n",
    "                # Acquire the lock for stacking\n",
    "                with locks[ind]:\n",
    "                    # Bin pair cross-correlograms in frequency domain\n",
    "                    spec_xcorr_stack[ind] += spec_xcorr[_]\n",
    "                    # Count the trace pairs in each inter-receiver bin\n",
    "                    bin_count[ind] += 1\n",
    "\n",
    "        # Run the cross-correlation function in a separate thread\n",
    "        for s1 in range(ntraces):\n",
    "            executor.submit(cross_correlation, s1)\n",
    "\n",
    "    # inverse FFT to get the cross-correlation function in time domain\n",
    "    image = np.fft.irfft(spec_xcorr_stack)\n",
    "    # fold the cross-correlation function and trim the time window\n",
    "    nmaxlag = int(max_tlag / spec_st[0].stats.delta)\n",
    "    image = 0.5 * (image + image[:, ::-1])[:, :nmaxlag]\n",
    "\n",
    "    # output in dictionary\n",
    "    correlogram = dict(bin_count=bin_count, # number of traces in each bin\n",
    "                  delta_t=spec_st[0].stats.delta, # seconds\n",
    "                  delta_d=dist_bins[1] - dist_bins[0], # degrees\n",
    "                  interstation_dist=(dist_bins[1:]+dist_bins[:-1])/2, # seconds\n",
    "                  time_lapse=np.arange(nmaxlag)*spec_st[0].stats.delta, # degrees\n",
    "                  image=image) # dimensionaless of size (nbins, nmaxlag)\n",
    "    # save the correlogram to a file if fname is given\n",
    "    if fname is not None:\n",
    "        with open(fname, 'wb') as f: pickle.dump(correlogram, f)\n",
    "    return correlogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3094,
     "status": "ok",
     "timestamp": 1744300915502,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "pbxwmDpjNC2_",
    "outputId": "7a7622cf-0a08-4d1e-e42a-a72111e4218e"
   },
   "outputs": [],
   "source": [
    "dist_bins = np.linspace(0, 180, 181) # degrees\n",
    "max_tlag = 7000 # seconds\n",
    "\n",
    "# Cross-correlation stacking of the spectral traces\n",
    "start = time.time()\n",
    "correlogram = xcorr_stack(spec_st, dist_bins, max_tlag)\n",
    "print (f'\\nCross-correlation stacking took {time.time()-start:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG4gBwC9NC2_"
   },
   "source": [
    "We define the following function to display the computed correlogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wm448a1eatM"
   },
   "outputs": [],
   "source": [
    "def display_correlogram(correlogram_or_fname, filter_kw, ymax=4000, title=None):\n",
    "    \"\"\"\n",
    "    Plot the cross-correlation function from the correlogram dictionary or file.\n",
    "    :param correlogram_or_fname: str or dict\n",
    "        The file name of the correlogram or the correlogram dictionary.\n",
    "    :param filter_kw: dict\n",
    "        The filter parameters for the cross-correlation function.\n",
    "    :param ymax: float\n",
    "        The maximum y-axis limit for the plot.\n",
    "    :param title: str\n",
    "        The title of the plot.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if type(correlogram_or_fname) is not dict:\n",
    "        with open(correlogram_or_fname, 'rb') as f: correlogram = pickle.load(f)\n",
    "    else:\n",
    "        from copy import deepcopy\n",
    "        correlogram = deepcopy(correlogram_or_fname)\n",
    "\n",
    "    # Get info from the correlogram dictionary\n",
    "    delta = correlogram['delta_t']\n",
    "    bin_count = correlogram['bin_count']\n",
    "    img = correlogram['image']\n",
    "    time_lapse = correlogram['time_lapse']\n",
    "    inter_dist = correlogram['interstation_dist']\n",
    "\n",
    "    # Normalize the cross-correlation function by the number of traces in each bin\n",
    "    from obspy.signal.filter import bandpass\n",
    "    for i in range(img.shape[0]):\n",
    "        if bin_count[i] == 0: continue\n",
    "        img[i] = bandpass(img[i], df=1/delta, freqmin=filter_kw['freqmin'], freqmax=filter_kw['freqmax'],\n",
    "                            corners=filter_kw['corners'], zerophase=filter_kw['zerophase'])\n",
    "        img[i] /= bin_count[i]\n",
    "\n",
    "    # Plot the cross-correlation function\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(3.5, 7), sharex=True, gridspec_kw={'height_ratios': (.2, 1.)})\n",
    "    ax[0].bar(np.arange(len(bin_count)), bin_count, width=1, color='gray', edgecolor='none')\n",
    "    ax[0].set(ylabel='# pairs', title=title)\n",
    "    VMAX = 2.5e-2 * np.max(np.abs(img))\n",
    "    ax[1].imshow(img.T, aspect='auto', vmin=-VMAX, vmax=VMAX, cmap='seismic', origin='lower',\n",
    "                interpolation='sinc', extent=(0, 180, time_lapse[0], time_lapse[-1]))\n",
    "    ax[1].set(ylim=(0, ymax), xlim=(0, 180),\n",
    "            xlabel='Inter-station distance (degrees)', ylabel='Time lapse (seconds)')\n",
    "    for x in ax: x.grid(ls='--', lw=.5, color='gray')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_correlogram_plain(correlogram_or_fname, filter_kw, ymax=4000, fname=None):\n",
    "    \"\"\"\n",
    "    Plot the cross-correlation function from the correlogram dictionary or file.\n",
    "    :param correlogram_or_fname: str or dict\n",
    "        The file name of the correlogram or the correlogram dictionary.\n",
    "    :param filter_kw: dict\n",
    "        The filter parameters for the cross-correlation function.\n",
    "    :param ymax: float\n",
    "        The maximum y-axis limit for the plot.\n",
    "    :param fname: str\n",
    "        Filename to save the plot.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if type(correlogram_or_fname) is not dict:\n",
    "        with open(correlogram_or_fname, 'rb') as f: correlogram = pickle.load(f)\n",
    "    else:\n",
    "        from copy import deepcopy\n",
    "        correlogram = deepcopy(correlogram_or_fname)\n",
    "\n",
    "    # Get info from the correlogram dictionary\n",
    "    delta = correlogram['delta_t']\n",
    "    bin_count = correlogram['bin_count']\n",
    "    img = correlogram['image']\n",
    "    time_lapse = correlogram['time_lapse']\n",
    "    inter_dist = correlogram['interstation_dist']\n",
    "\n",
    "    # Normalize the cross-correlation function by the number of traces in each bin\n",
    "    from obspy.signal.filter import bandpass\n",
    "    for i in range(img.shape[0]):\n",
    "        if bin_count[i] == 0: continue\n",
    "        img[i] = bandpass(img[i], df=1/delta, freqmin=filter_kw['freqmin'], freqmax=filter_kw['freqmax'],\n",
    "                            corners=filter_kw['corners'], zerophase=filter_kw['zerophase'])\n",
    "        img[i] /= bin_count[i]\n",
    "\n",
    "    # Plot the cross-correlation function\n",
    "    fig, ax = plt.subplots(figsize=(2.5, 5))\n",
    "    VMAX = 2.5e-2 * np.max(np.abs(img))\n",
    "    ax.imshow(img.T, aspect='auto', vmin=-VMAX, vmax=VMAX, cmap='seismic', origin='lower',\n",
    "                interpolation='sinc', extent=(0, 180, time_lapse[0], time_lapse[-1]))\n",
    "    ax.set(xticklabels=[], yticklabels=[], xlim=(0, 180), ylim=(0, ymax))\n",
    "    ax.grid(ls='--', lw=.5, color='gray')\n",
    "    fig.tight_layout()\n",
    "    if fname is not None: plt.savefig(fname, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "executionInfo": {
     "elapsed": 4548,
     "status": "ok",
     "timestamp": 1744300926452,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "h7gRYvnsNC3B",
    "outputId": "80924273-6db5-46ba-fea2-d70b71e071f6"
   },
   "outputs": [],
   "source": [
    "display_correlogram(correlogram, filter_kw, title=f'Event ID: {isc_eventid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqKr8B95akfZ"
   },
   "source": [
    "The above processing has been the backbone data processing for multiple publications from the ANU Global Seismology group using the concept of the global correlation wavefield studying the deep Earth interiors, including,\n",
    "- Tkalčić H., T.-S. Phạm and S. Wang, The Earth's coda correlation wavefield: Rise of the new paradigm and recent advances, Earth-Science Reviews, 208, doi:10.1016/j.earscirev.2020.103285, 2020.\n",
    "- Phạm T.-S., H. Tkalčić, M. Sambridge, and B. L. N. Kennett, Earth's correlation wavefield: Late coda correlation, Geophys. Res. Lett., 45 (7),  doi:10.1002/2018GL077244, 2018.\n",
    "- Tkalčić, H. and T.-S. Phạm, Shear properties of the Earth's inner core revealed by a detection of J waves in global correlation wavefield, Science, 362, doi:10.1126/science.aau7649, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_-sHosEeatN"
   },
   "source": [
    "---\n",
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQuRt2tWakfZ"
   },
   "source": [
    "### Student work\n",
    "\n",
    "Below is the list of 12 grade A events according to Tkalčić & Phạm ([2020](https://doi.org/10.1093/gji/ggaa369)). These events were empirically found to be effective in excite clear features in the global correlogram individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1744300929696,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "Mi7EWc3-NC3D",
    "outputId": "215ebd72-afbc-430f-bc41-1b54ac597fa3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/tsonpham/ObsSeisHUS2025/master/Day4/GradeA_events.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXf0P_F9akfa"
   },
   "source": [
    "You are challenged to compute the individual global correlogram of one or more events from the list using their `isc_eventid`. Please copy this piece of code to the next cell and complete it with correct keywords.\n",
    "\n",
    "```\n",
    "isc_eventid = <CHANGE ME>\n",
    "# Fetch the event information\n",
    "event = get_event(isc_eventid)\n",
    "\n",
    "# Download the waveforms from IRIS unless they existed locally\n",
    "dstream = get_waveform_data(<CHANGE ME>, network, channel, window_start, window_end)\n",
    "\n",
    "# Do some simple data processing: trim the waveform at desired time window and resample 1 Hz\n",
    "dstream = data_processing(dstream, channel, <CHANGE ME>, window_start, window_end)\n",
    "\n",
    "# Process single station data\n",
    "spec_st = <CHANGE ME>\n",
    "\n",
    "# Specify cross-correlation parameters\n",
    "corr_fname = event.preferred_origin().time.strftime(f'XC.%Y%m%d.pkl')\n",
    "xcorr_stack(spec_st, <CHANGE ME>, <CHANGE ME>, fname=corr_fname)\n",
    "\n",
    "# Plot the cross-correlation function\n",
    "display_correlogram(corr_fname, filter_kw, title=f'Event ID: {isc_eventid}')\n",
    "```\n",
    "\n",
    "Please upload your XC.????????.h5 to this shared Google Drive [directory](https://drive.google.com/drive/folders/1PP5UQ2Wyam82Db9zZxZ06cwmNTx6ZFlv?usp=drive_link). We will see how the global correlogram stacked by multiple events look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZmgq26QNC3C"
   },
   "outputs": [],
   "source": [
    "## Enter your code here, replace <CHANGE ME> with correct code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "executionInfo": {
     "elapsed": 7443,
     "status": "ok",
     "timestamp": 1744301271850,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "FDdemdfzNC3E",
    "outputId": "8f0ac3d0-8e9c-4e56-e6f3-a1e3b040ae4d"
   },
   "outputs": [],
   "source": [
    "#@title View the solution\n",
    "#@markdown Try to complete the code before running this cell.\n",
    "\n",
    "isc_eventid = \"611831635\"\n",
    "# Fetch the event information\n",
    "event = get_event(isc_eventid)\n",
    "\n",
    "# Download the waveforms from IRIS unless they existed locally\n",
    "dstream = get_waveform_data(event.preferred_origin(), network, channel, window_start, window_end)\n",
    "\n",
    "# Do some simple data processing: trim the waveform at desired time window and resample 1 Hz\n",
    "dstream = data_processing(dstream, channel, event.preferred_origin().time, window_start, window_end)\n",
    "\n",
    "# Process single station data\n",
    "spec_st = single_station_processing(dstream)\n",
    "\n",
    "# Specify cross-correlation parameters\n",
    "corr_fname = event.preferred_origin().time.strftime(f'XC.%Y%m%d.pkl')\n",
    "xcorr_stack(spec_st, dist_bins, max_tlag, fname=corr_fname)\n",
    "\n",
    "# Plot the cross-correlation function\n",
    "display_correlogram(corr_fname, filter_kw, title=f'Event ID: {isc_eventid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2-tJTE-akfa"
   },
   "source": [
    "### Lecturer's space\n",
    "The lecturer will run the next cells to compute the stacked correlogram from the updated individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q5h9sT2YA2z"
   },
   "outputs": [],
   "source": [
    "#@title Mount Google Drive (optional)\n",
    "#@markdown Mount Google Drive when running in Google Colab to access the computed correlograms.\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_MOUNTED = True\n",
    "    XCORR_path = Path('/content/drive/Othercomputers/My MacBook Pro/HUS_Workshop/ObsSeisHUS2025/Day4/XCORR_PUB')\n",
    "    if not XCORR_path.exists():\n",
    "        XCORR_path = Path('/content/XCORR')\n",
    "        XCORR_path.mkdir(exist_ok=True)\n",
    "except ImportError:\n",
    "    DRIVE_MOUNTED = False\n",
    "    # XCORR_path = Path('XCORR_PUB')\n",
    "    XCORR_path = Path('XCORR')\n",
    "    XCORR_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1744300988962,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "eRa3vBp8X4KH",
    "outputId": "785a4fae-ce5c-41fe-bda8-83c16b624b7b"
   },
   "outputs": [],
   "source": [
    "#@title Download and process the cross-correlation function for all events in the list\n",
    "#@markdown This will take a while to run. You can run this in the background and come back later.\n",
    "\n",
    "# for isc_eventid in list(df['isc_eventid']):\n",
    "#     # Fetch the event information\n",
    "#     event = get_event(isc_eventid)\n",
    "#     corr_fname = XCORR_path/event.preferred_origin().time.strftime('XC.%Y%m%d.pkl')\n",
    "#     if corr_fname.exists():\n",
    "#         print (f'File {corr_fname} already exists. Skipping ...'); continue\n",
    "#     print (f'Processing event {isc_eventid} ...')\n",
    "\n",
    "#     # Download the waveforms from IRIS unless they existed locally\n",
    "#     dstream = get_waveform_data(event.preferred_origin(), network, channel, window_start, window_end)\n",
    "#     # Do some simple data processing: trim the waveform at desired time window and resample 1 Hz\n",
    "#     dstream = data_processing(dstream, channel, event.preferred_origin().time, window_start, window_end)\n",
    "#     # Process single station data\n",
    "#     spec_st = single_station_processing(dstream)\n",
    "#     # Specify cross-correlation parameters\n",
    "#     xcorr_stack(spec_st, dist_bins, max_tlag, fname=corr_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "executionInfo": {
     "elapsed": 6497,
     "status": "ok",
     "timestamp": 1744301469877,
     "user": {
      "displayName": "Thanh Son Pham",
      "userId": "17941529104681711853"
     },
     "user_tz": -420
    },
    "id": "kv3akMocNC3F",
    "outputId": "c55cf1e8-650c-409e-832d-1d39865f4230"
   },
   "outputs": [],
   "source": [
    "#@title Plot the stacked cross-correlogram\n",
    "#@markdown This cell with will stack all the submited correlogram and display the result.\n",
    "\n",
    "def stack_correlograms(fname_list):\n",
    "    \"\"\"\n",
    "    Stack the cross-correlograms from the given list of files.\n",
    "    \"\"\"\n",
    "    # load the correlograms\n",
    "    fname_list = list(fname_list)\n",
    "    correlograms = [pickle.load(open(fname, 'rb')) for fname in fname_list]\n",
    "    # display the individual correlogram\n",
    "    # for fname in fname_list: display_correlogram_plain(fname, filter_kw, fname=str(fname).replace('.pkl', '.png'))\n",
    "    # for i, c in enumerate(correlograms):\n",
    "    #     print (fname_list[i], c.keys())\n",
    "    # get the number of bins and time lags\n",
    "    bin_count = np.sum([c['bin_count'] for c in correlograms], axis=0)\n",
    "    image = np.sum([c['image'] for c in correlograms], axis=0)\n",
    "\n",
    "    output = correlograms[0].copy()\n",
    "    output['bin_count'] = bin_count\n",
    "    output['image'] = image\n",
    "    return output\n",
    "\n",
    "display_correlogram(stack_correlograms(XCORR_path.glob('*.pkl')), filter_kw, title='Stacked cross-correlogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "simae3UpGDyL"
   },
   "source": [
    "---\n",
    "## Conclusions\n",
    "- The data processing presented in this notebook turns noisy data into valuable insights into the Earth's deep interior.\n",
    "- This framework has enabled several discoveries of the deep Earth and planetary interiors at the ANU Global seismology group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus project\n",
    "\n",
    "Hello, thank you for being interested in the bonus project. Although everyone is encouraged to complete bonus projects, undergrad students will be considered for bonus points to their final exams. Best of luck!\n",
    "\n",
    "*In this project, you are asked to write a small piece of code to compile the list of all ISC event ids for M > 7.0 in year 2015, compute their single-event correlograms, and finally display the stacked correlograms.*\n",
    "\n",
    "If you submit a working code towards completing the task, you will get 50% points. If the code produces correct outcome, you will get 75%. The minimal two-paragraphs on the motivation and additional thoughts will get you to 100%.\n",
    "\n",
    "Please submit this jupyter notebook to the following form: https://forms.gle/L5QLLYMEnm277bTMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click for hint\n",
    "\n",
    "## Uncomment the code to retrieve the ISC event IDs for event of magnitudes>7.5 in year 2015.\n",
    "cat2015 = Client('ISC').get_events(starttime=UTCDateTime(2015, 1, 1), endtime=UTCDateTime(2015, 12, 31),\n",
    "    minmagnitude=7.5, maxmagnitude=10.0)\n",
    "isc_eventid_2015 = [ev.resource_id.id.split('=')[-1] for ev in cat2015]\n",
    "print (isc_eventid_2015)\n",
    "\n",
    "## Interatively compute individual event correlograms.\n",
    "corr_fname_2015 = []\n",
    "for isc_eventid in isc_eventid_2015:\n",
    "    # Fetch the event information\n",
    "    event = get_event(isc_eventid)\n",
    "    corr_fname = XCORR_path/event.preferred_origin().time.strftime('XC.%Y%m%d.pkl')\n",
    "    corr_fname_2015.append(corr_fname)\n",
    "    if corr_fname.exists():\n",
    "        print (f'File {corr_fname} already exists. Skipping ...'); continue\n",
    "    print (f'Processing event {isc_eventid} ...')\n",
    "\n",
    "    ## ENTER YOUR CODE HERE\n",
    "\n",
    "## Stack the computed correlograms and plot the result.\n",
    "## ENTER YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "emcee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
